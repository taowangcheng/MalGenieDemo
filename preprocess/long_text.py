import contractions
import re
from enchant.tokenize import get_tokenizer, HTMLChunker, EmailFilter, URLFilter
from nltk import WordNetLemmatizer, pos_tag
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk import sent_tokenize

code_block_replace = re.compile(r'(`+)(.*)(`+)', re.S)
image_block_replace = re.compile(r'!\[(.*)]\((.*)\)', re.S)

# ä¸“æœ‰åè¯å¤„ç†
# from nltk import MWETokenizer
# proper_nouns = [('spring', 'boot')]
# multi_word_tk = MWETokenizer(proper_nouns, separator=' ')

# åˆ†è¯å™¨é™„åŠ htmlæå–ï¼Œemailè¿‡æ»¤ï¼Œurlè¿‡æ»¤
word_tk = get_tokenizer("en_US", chunkers=(HTMLChunker,), filters=(EmailFilter, URLFilter))
word_lemma = WordNetLemmatizer()
stop_words = stopwords.words('english')
stop_words.append('cannot')
# stop_words.append('youtube')
stop_words = set(stop_words)  # æé«˜æŸ¥è¯¢é€Ÿåº¦


# è¯æ€§è·å–
def get_word_tag(word_tag):
    if word_tag.startswith('J'):
        return wordnet.ADJ
    elif word_tag.startswith('V'):
        return wordnet.VERB
    elif word_tag.startswith('N'):
        return wordnet.NOUN
    elif word_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None


def process(doc):
    if doc:
        doc = code_block_replace.sub('\n', doc)
        doc = image_block_replace.sub('\t', doc)
        # doc = doc.encode('ascii', 'ignore').decode()
        doc = doc.encode('ascii', 'ignore').decode().replace("http", " http")
        doc = contractions.fix(doc, slang=True)     # å°†ç¼©ç•¥è¯å±•å¼€
        doc = sent_tokenize(doc.lower())
        word_list = []
        for sentence in doc:
            sentence_temp = []
            for (word, pos) in word_tk(sentence):
                sentence_temp.append(word)
            sentence = sentence_temp
            # sentence = multi_word_tk.tokenize(sentence)  # æ‹†åˆ†ä¹‹åå°†ä¸“æœ‰è¯æ‹¼æ¥
            tagged_sent = pos_tag(sentence)  # è·å–å•è¯è¯æ€§
            lemmaed_sent = []
            for tag in tagged_sent:
                wordnet_pos = get_word_tag(tag[1]) or wordnet.NOUN  # æŒ‰ä½è¿ç®—
                lemmaed_sent.append(word_lemma.lemmatize(tag[0], pos=wordnet_pos))  # è¯å½¢è¿˜åŸ
            sentence = [word for word in lemmaed_sent if word not in stop_words]
            sentence_temp = []
            for word in sentence:
                temp = word.split("'")
                sentence_temp += temp
            sentence_final = [word for word in sentence_temp if len(word) > 1]
            word_list.append(sentence_final)
    else:
        word_list = [[]]
    return word_list


if __name__ == '__main__':
    texts = ["![è¿™æ˜¯å›¾ç‰‡](/assets/img/philly-magic-garden.jpg \"Magic Gardens\")",
             " ğŸš€ The ultimate project builder for Astro  ğŸš€                                                                                      âš ï¸ WARNING: In development âš ï¸"]
    for text in texts:
        print(text, ": ", process(text))